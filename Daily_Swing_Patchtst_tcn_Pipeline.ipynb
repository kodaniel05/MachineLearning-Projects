{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb187e9a",
   "metadata": {},
   "source": [
    "# Deep Trading Model — PatchTST / TCN (PyTorch • Apple Silicon MPS) - Personal\n",
    "**Date:** 2025-09-10\n",
    "\n",
    "End-to-end **daily swing** pipeline with **deep models**: **PatchTST Transformer** or **TCN**. Includes:\n",
    "- Heavier **feature set** (momentum/vol/trend, VWAP-like proxies, cross-sectional ranks, rolling betas/correlations, Garman–Klass & Yang–Zhang vol).\n",
    "- **Triple-barrier meta-label** (trade/no-trade) + **return regression** head.\n",
    "- **Masked-patch pretraining** (optional) to soak compute, then supervised fine-tune.\n",
    "- **Gradient checkpointing**, **AMP**, **EMA**, **Cosine LR**, **torch.compile** (safe try).\n",
    "- **Cost-aware backtest** with your risk rules (prob ≥ 0.55, top-K, ATR TP/SL, –2% day, –6% week pause).\n",
    "\n",
    "> Default data via Yahoo Finance (EOD) for the ETF universe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7caf717a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.7\n",
      "PyTorch: 2.8.0+cu128\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import torch, platform, os, json, random, numpy as np\n",
    "print('Python:', platform.python_version())\n",
    "print('PyTorch:', torch.__version__)\n",
    "has_mps = torch.backends.mps.is_available() and torch.backends.mps.is_built()\n",
    "has_cuda = torch.cuda.is_available()\n",
    "device = torch.device('mps' if has_mps else ('cuda' if has_cuda else 'cpu'))\n",
    "print('Device:', device)\n",
    "try:\n",
    "    torch.set_float32_matmul_precision('medium')\n",
    "except Exception as e:\n",
    "    print('matmul precision setting skipped:', e)\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(suppress=True, linewidth=120)\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "SAVE_DIR = 'artifacts_pro'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2ebdfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"MODEL_TYPE\": \"patchtst\",\n",
      "  \"SEQ_LEN\": 384,\n",
      "  \"PATCH_LEN\": 16,\n",
      "  \"PATCH_STRIDE\": 8,\n",
      "  \"DMODEL\": 320,\n",
      "  \"N_LAYERS\": 6,\n",
      "  \"BATCH_SIZE\": 128,\n",
      "  \"EPOCHS\": 40,\n",
      "  \"DO_PRETRAIN\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    # Data\n",
    "    'UNIVERSE': ['SPY','QQQ','IWM','DIA','XLK','XLF','XLE','XLV','XLY','XLP','XLI','XLB','XLRE','XLU','GLD','TLT','HYG'],\n",
    "    'START_DATE': '2010-01-01',\n",
    "    'VAL_START':  '2023-01-01',\n",
    "    'TEST_START': '2024-01-01',\n",
    "    'END_DATE':   None,\n",
    "    'DATA_SOURCE': 'yfinance',         # 'yfinance' or 'local'\n",
    "    'LOCAL_PATH': 'TODO/path/to/csvs', # if local\n",
    "\n",
    "    # Labeling / trading\n",
    "    'RET_HORIZON': 5,           # k-day forward regression label\n",
    "    'TB_ATR_TP':   2.5,         # triple-barrier TP multiple of ATR%\n",
    "    'TB_ATR_SL':   1.75,        # triple-barrier SL multiple of ATR%\n",
    "    'TB_TIMEOUT':  5,           # days\n",
    "\n",
    "    # Features\n",
    "    'RSI_PERIODS': [3,14],\n",
    "    'MA_WINDOWS':  [20,50,200],\n",
    "    'ATR_WINDOW':  14,\n",
    "    'RET_LAGS':    [1,3,5,10,20],\n",
    "    'BETA_WINDOW': 60,\n",
    "    'CORR_WINDOW': 60,\n",
    "\n",
    "    # Sequence (for deep models)\n",
    "    'SEQ_LEN':     384, # total timesteps per sample\n",
    "    'PATCH_LEN':   16, # for PatchTST\n",
    "    'PATCH_STRIDE': 8,\n",
    "\n",
    "    # Model choice\n",
    "    'MODEL_TYPE':  'patchtst',  # 'patchtst' or 'tcn'\n",
    "\n",
    "    # PatchTST\n",
    "    'DMODEL':      320,\n",
    "    'N_HEADS':     8,\n",
    "    'N_LAYERS':    6,\n",
    "    'DROPOUT':     0.15,\n",
    "\n",
    "    # TCN\n",
    "    'TCN_CHANNELS': [128,128,256,256,256,256,256,256],\n",
    "    'TCN_KERNEL':   7,\n",
    "    'TCN_DROPOUT':  0.1,\n",
    "\n",
    "    # Train\n",
    "    'BATCH_SIZE':  128,\n",
    "    'LR':          1e-3,\n",
    "    'WEIGHT_DECAY':1e-4,\n",
    "    'EPOCHS':      40,\n",
    "    'PATIENCE':    8,\n",
    "    'GRAD_CLIP':   1.0,\n",
    "    'ACCUM_STEPS': 1, # gradient accumulation\n",
    "\n",
    "    # Pretraining\n",
    "    'DO_PRETRAIN': True,\n",
    "    'PRE_EPOCHS':  15,\n",
    "    'MASK_PROB':   0.3, # fraction of patches masked for reconstruction\n",
    "\n",
    "    # Loss mixing\n",
    "    'LOSS_META_W': 0.6,\n",
    "    'LOSS_RET_W':  0.4,\n",
    "\n",
    "    # Backtest rules\n",
    "    'PROB_CUTOFF': 0.55,\n",
    "    'TOP_K':       5,\n",
    "    'HOLD_MAX_DAYS': 5,\n",
    "    'ATR_MULT_STOP': 1.75,\n",
    "    'ATR_MULT_TP':   2.5,\n",
    "    'START_EQUITY':  25_000.0,\n",
    "    'MAX_GROSS_LEV': 1.5,\n",
    "    'PER_NAME_CAP':  0.125,\n",
    "    'MAX_NET_EXP':   0.80,\n",
    "    'DAILY_LOSS_CAP': -0.02,\n",
    "    'WEEKLY_LOSS_CAP': -0.06,\n",
    "    'PAUSE_DAYS_ON_WEEKLY_BREACH': 3,\n",
    "    'SLIPPAGE_BPS': 3,\n",
    "    'FEE_BPS_ROUNDTRIP': 1,\n",
    "    'COMMISSION_PER_SHARE': 0.0035,\n",
    "    'COMMISSION_MIN': 0.35,\n",
    "}\n",
    "print(json.dumps({k:CONFIG[k] for k in ['MODEL_TYPE','SEQ_LEN','PATCH_LEN','PATCH_STRIDE','DMODEL','N_LAYERS','BATCH_SIZE','EPOCHS','DO_PRETRAIN']}, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfe247db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for data & modeling\n",
    "import math, time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, mean_absolute_error, mean_squared_error, accuracy_score, f1_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "try:\n",
    "    import yfinance as yf\n",
    "except Exception as e:\n",
    "    print('Install yfinance if using DATA_SOURCE=yfinance: pip install yfinance')\n",
    "\n",
    "from collections import defaultdict, deque\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "import torch.utils.checkpoint as cp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e993af1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OHLCV shape: (67269, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2025-09-25</th>\n",
       "      <th>XLU</th>\n",
       "      <td>86.169998</td>\n",
       "      <td>86.349998</td>\n",
       "      <td>85.419998</td>\n",
       "      <td>85.419998</td>\n",
       "      <td>8971346.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLV</th>\n",
       "      <td>136.125000</td>\n",
       "      <td>136.179993</td>\n",
       "      <td>133.725006</td>\n",
       "      <td>134.130005</td>\n",
       "      <td>14085817.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLY</th>\n",
       "      <td>238.110001</td>\n",
       "      <td>238.195007</td>\n",
       "      <td>235.160004</td>\n",
       "      <td>236.220001</td>\n",
       "      <td>5717212.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Open        High         Low       Close      Volume\n",
       "Date       Symbol                                                            \n",
       "2025-09-25 XLU      86.169998   86.349998   85.419998   85.419998   8971346.0\n",
       "           XLV     136.125000  136.179993  133.725006  134.130005  14085817.0\n",
       "           XLY     238.110001  238.195007  235.160004  236.220001   5717212.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Ingest\n",
    "def load_data_yf(symbols, start, end=None):\n",
    "    df = yf.download(symbols, start=start, end=end, auto_adjust=True, progress=False, group_by='ticker')\n",
    "    frames = []\n",
    "    for s in symbols:\n",
    "        sub = df[s].copy()\n",
    "        sub.columns = [c.capitalize() for c in sub.columns]  # Open, High, Low, Close, Volume\n",
    "        sub['Symbol'] = s\n",
    "        frames.append(sub.reset_index().set_index(['Date','Symbol']))\n",
    "    out = pd.concat(frames).sort_index()\n",
    "    return out\n",
    "\n",
    "def load_data_local(folder, symbols):\n",
    "    import os\n",
    "    frames = []\n",
    "    for s in symbols:\n",
    "        path = os.path.join(folder, f\"{s}.csv\")\n",
    "        sub = pd.read_csv(path, parse_dates=['Date'])\n",
    "        sub['Symbol'] = s\n",
    "        sub = sub[['Date','Symbol','Open','High','Low','Close','Volume']]\n",
    "        frames.append(sub.set_index(['Date','Symbol']))\n",
    "    out = pd.concat(frames).sort_index()\n",
    "    return out\n",
    "\n",
    "if CONFIG['DATA_SOURCE']=='yfinance':\n",
    "    ohlcv = load_data_yf(CONFIG['UNIVERSE'], CONFIG['START_DATE'], CONFIG['END_DATE'])\n",
    "else:\n",
    "    ohlcv = load_data_local(CONFIG['LOCAL_PATH'], CONFIG['UNIVERSE'])\n",
    "print('OHLCV shape:', ohlcv.shape); display(ohlcv.tail(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "267529d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (62418, 33)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ret1</th>\n",
       "      <th>Ret_1</th>\n",
       "      <th>Ret_3</th>\n",
       "      <th>Ret_5</th>\n",
       "      <th>Ret_10</th>\n",
       "      <th>...</th>\n",
       "      <th>DOW_1</th>\n",
       "      <th>DOW_2</th>\n",
       "      <th>DOW_3</th>\n",
       "      <th>DOW_4</th>\n",
       "      <th>SPY_Regime</th>\n",
       "      <th>BetaSPY</th>\n",
       "      <th>CorrSPY</th>\n",
       "      <th>Ret_10_Rank</th>\n",
       "      <th>RV10_Rank</th>\n",
       "      <th>DistMA_20_Rank</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2025-09-25</th>\n",
       "      <th>XLU</th>\n",
       "      <td>86.169998</td>\n",
       "      <td>86.349998</td>\n",
       "      <td>85.419998</td>\n",
       "      <td>85.419998</td>\n",
       "      <td>8971346.0</td>\n",
       "      <td>-0.009623</td>\n",
       "      <td>-0.009623</td>\n",
       "      <td>0.002582</td>\n",
       "      <td>0.018777</td>\n",
       "      <td>0.010873</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0.300225</td>\n",
       "      <td>0.215991</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLV</th>\n",
       "      <td>136.125000</td>\n",
       "      <td>136.179993</td>\n",
       "      <td>133.725006</td>\n",
       "      <td>134.130005</td>\n",
       "      <td>14085817.0</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>-0.018298</td>\n",
       "      <td>-0.019834</td>\n",
       "      <td>-0.035618</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0.504096</td>\n",
       "      <td>0.296005</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.176471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XLY</th>\n",
       "      <td>238.110001</td>\n",
       "      <td>238.195007</td>\n",
       "      <td>235.160004</td>\n",
       "      <td>236.220001</td>\n",
       "      <td>5717212.0</td>\n",
       "      <td>-0.014107</td>\n",
       "      <td>-0.014107</td>\n",
       "      <td>-0.016488</td>\n",
       "      <td>-0.015982</td>\n",
       "      <td>-0.003215</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>1.261185</td>\n",
       "      <td>0.739777</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.352941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Open        High         Low       Close      Volume  \\\n",
       "Date       Symbol                                                               \n",
       "2025-09-25 XLU      86.169998   86.349998   85.419998   85.419998   8971346.0   \n",
       "           XLV     136.125000  136.179993  133.725006  134.130005  14085817.0   \n",
       "           XLY     238.110001  238.195007  235.160004  236.220001   5717212.0   \n",
       "\n",
       "                       Ret1     Ret_1     Ret_3     Ret_5    Ret_10  ...  \\\n",
       "Date       Symbol                                                    ...   \n",
       "2025-09-25 XLU    -0.009623 -0.009623  0.002582  0.018777  0.010873  ...   \n",
       "           XLV    -0.016570 -0.016570 -0.018298 -0.019834 -0.035618  ...   \n",
       "           XLY    -0.014107 -0.014107 -0.016488 -0.015982 -0.003215  ...   \n",
       "\n",
       "                   DOW_1  DOW_2  DOW_3  DOW_4  SPY_Regime   BetaSPY   CorrSPY  \\\n",
       "Date       Symbol                                                               \n",
       "2025-09-25 XLU     False  False   True  False           1  0.300225  0.215991   \n",
       "           XLV     False  False   True  False           1  0.504096  0.296005   \n",
       "           XLY     False  False   True  False           1  1.261185  0.739777   \n",
       "\n",
       "                   Ret_10_Rank  RV10_Rank  DistMA_20_Rank  \n",
       "Date       Symbol                                          \n",
       "2025-09-25 XLU        0.764706   0.823529        0.823529  \n",
       "           XLV        0.117647   0.588235        0.176471  \n",
       "           XLY        0.529412   0.705882        0.352941  \n",
       "\n",
       "[3 rows x 33 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature Engineering (heavy but leakage-safe)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def rsi(series, window=14):\n",
    "    delta = series.diff()\n",
    "    up = delta.clip(lower=0.0); dn = (-delta).clip(lower=0.0)\n",
    "    ru = up.rolling(window).mean(); rd = dn.rolling(window).mean()\n",
    "    rs = ru / (rd + 1e-12)\n",
    "    return 100 - 100/(1+rs)\n",
    "\n",
    "def atr(high, low, close, window=14):\n",
    "    prev_close = close.shift(1)\n",
    "    tr = pd.concat([high-low, (high-prev_close).abs(), (low-prev_close).abs()], axis=1).max(axis=1)\n",
    "    return tr.rolling(window).mean()\n",
    "\n",
    "def garman_klass(o,h,l,c, window=20):\n",
    "    rs = 0.5*(np.log(h/l))**2 - (2*np.log(2)-1)*(np.log(c/o))**2\n",
    "    return pd.Series(rs, index=c.index).rolling(window).sum().pow(0.5)\n",
    "\n",
    "def yang_zhang(o,h,l,c, window=20):\n",
    "    k = 0.34/(1.34+(window+1)/(window-1))\n",
    "    oc = np.log(o.shift(1)/c.shift(1))\n",
    "    co = np.log(c/o)\n",
    "    hl = np.log(h/l)\n",
    "    rs = oc.rolling(window).var() + k*hl.rolling(window).mean() + (1-k)*co.rolling(window).var()\n",
    "    return rs.pow(0.5)\n",
    "\n",
    "def realized_vol(ret, window=10):\n",
    "    return ret.rolling(window).std()*np.sqrt(252)\n",
    "\n",
    "def build_features(df, cfg):\n",
    "    # Expect MultiIndex index: ['Date','Symbol'] and columns: Open, High, Low, Close\n",
    "    x = df.copy().sort_index()\n",
    "\n",
    "    # 1) Simple returns and lags\n",
    "    x['Ret1'] = x['Close'].groupby(level='Symbol').pct_change()\n",
    "    for L in cfg['RET_LAGS']:\n",
    "        x[f'Ret_{L}'] = x['Close'].groupby(level='Symbol').pct_change(L)\n",
    "\n",
    "    # 2) RSI family\n",
    "    for R in cfg['RSI_PERIODS']:\n",
    "        x[f'RSI_{R}'] = (\n",
    "            x.groupby(level='Symbol')['Close']\n",
    "              .transform(lambda s: rsi(s, R))\n",
    "        )\n",
    "\n",
    "    # 3) ATR (percent-like raw ATR — divide by Close if desired)\n",
    "    x['ATR'] = (\n",
    "        x.groupby(level='Symbol')\n",
    "          .apply(lambda g: atr(g['High'], g['Low'], g['Close'], cfg['ATR_WINDOW']))\n",
    "          .droplevel(0)  # drop Symbol level added by groupby-apply\n",
    "    )\n",
    "\n",
    "    # 4) Moving-average distances & slopes\n",
    "    for W in cfg['MA_WINDOWS']:\n",
    "        ma = x.groupby(level='Symbol')['Close'].transform(lambda s: s.rolling(W).mean())\n",
    "        x[f'DistMA_{W}']  = (x['Close'] - ma) / (ma + 1e-12)\n",
    "        x[f'SlopeMA_{W}'] = ma.groupby(level='Symbol').diff()\n",
    "\n",
    "    # 5) GK & YZ volatility (window=20)\n",
    "    x['GK20'] = (\n",
    "        x.groupby(level='Symbol')\n",
    "         .apply(lambda g: garman_klass(g['Open'], g['High'], g['Low'], g['Close'], 20))\n",
    "         .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    x['YZ20'] = (\n",
    "        x.groupby(level='Symbol')\n",
    "         .apply(lambda g: yang_zhang(g['Open'], g['High'], g['Low'], g['Close'], 20))\n",
    "         .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    # 6) Realized vol on Ret1\n",
    "    x['RV10'] = x.groupby(level='Symbol')['Ret1'].transform(lambda s: realized_vol(s, 10))\n",
    "\n",
    "    # 7) Day of week dummies\n",
    "    xi = x.reset_index()\n",
    "    xi['DOW'] = xi['Date'].dt.dayofweek\n",
    "    dummies = pd.get_dummies(xi['DOW'], prefix='DOW', drop_first=True)\n",
    "    xi = pd.concat([xi.drop(columns=['DOW']), dummies], axis=1).set_index(['Date','Symbol']).sort_index()\n",
    "    x = xi\n",
    "\n",
    "    # 8) Market regime from SPY (join on Date to all symbols)\n",
    "    spy = x.xs('SPY', level='Symbol', drop_level=False).copy()\n",
    "    spy_ma50 = spy['Close'].rolling(50).mean()\n",
    "    regime = (spy['Close'] >= spy_ma50).astype(int).rename('SPY_Regime')\n",
    "    # regime index is (Date, Symbol='SPY'); reduce to Date, then join\n",
    "    regime_by_date = regime.reset_index('Symbol', drop=True)\n",
    "    x = x.join(regime_by_date, on='Date')\n",
    "\n",
    "    # 9) Rolling beta & corr to SPY (cross-sectional, per date)\n",
    "    close = x['Close'].unstack('Symbol')\n",
    "    ret = close.pct_change()\n",
    "    spy_ret = ret['SPY']\n",
    "    def roll_corr_beta(s, ref, win):\n",
    "        cov = s.rolling(win).cov(ref)\n",
    "        var = ref.rolling(win).var()\n",
    "        beta = cov/(var + 1e-12)\n",
    "        corr = s.rolling(win).corr(ref)\n",
    "        return beta, corr\n",
    "\n",
    "    betas = pd.DataFrame(index=ret.index, columns=ret.columns, dtype=float)\n",
    "    corrs = pd.DataFrame(index=ret.index, columns=ret.columns, dtype=float)\n",
    "    for sym in ret.columns:\n",
    "        b, c = roll_corr_beta(ret[sym], spy_ret, cfg['BETA_WINDOW'])\n",
    "        betas[sym] = b; corrs[sym] = c\n",
    "\n",
    "    beta_long = betas.stack().rename('BetaSPY')\n",
    "    corr_long = corrs.stack().rename('CorrSPY')\n",
    "    x = x.join(beta_long, how='left').join(corr_long, how='left')\n",
    "\n",
    "    # 10) Cross-sectional ranks per day (ensure the columns exist given cfg)\n",
    "    def add_cs_ranks(frame, cols):\n",
    "        frame = frame.copy()\n",
    "        for col in cols:\n",
    "            if col not in frame.columns:\n",
    "                continue\n",
    "            pivot = frame[col].unstack('Symbol')\n",
    "            ranks = pivot.rank(axis=1, pct=True)\n",
    "            frame[f'{col}_Rank'] = ranks.stack()\n",
    "        return frame\n",
    "\n",
    "    # Example rank set — adjust to your cfg contents\n",
    "    rank_cols = []\n",
    "    if any(L == 10 for L in cfg['RET_LAGS']): rank_cols.append('Ret_10')\n",
    "    rank_cols += ['RV10']\n",
    "    if any(W == 20 for W in cfg['MA_WINDOWS']): rank_cols.append('DistMA_20')\n",
    "    x = add_cs_ranks(x, rank_cols)\n",
    "\n",
    "    # 11) Final clean\n",
    "    x = x.dropna().sort_index()\n",
    "    return x\n",
    "\n",
    "# Example usage (unchanged)\n",
    "features = build_features(ohlcv, CONFIG)\n",
    "print('Features shape:', features.shape)\n",
    "display(features.tail(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd0a63b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled rows: 62333\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>y_meta</th>\n",
       "      <th>y_ret</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2010-10-19</th>\n",
       "      <th>DIA</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.017329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLD</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.005901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HYG</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   y_meta     y_ret\n",
       "Date       Symbol                  \n",
       "2010-10-19 DIA        1.0  0.017329\n",
       "           GLD        1.0  0.005901\n",
       "           HYG        1.0  0.011526"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def triple_barrier_labels(df, cfg):\n",
    "    \"\"\"\n",
    "    df: MultiIndex [Date, Symbol] with columns at least: Open, High, Low, Close.\n",
    "        Optionally has ATR or ATR_Pct.\n",
    "    cfg: expects keys TB_ATR_TP, TB_ATR_SL, TB_TIMEOUT, RET_HORIZON, ATR_WINDOW\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Ensure sorted for forward slices\n",
    "    df = df.sort_index()\n",
    "\n",
    "    # Ensure ATR_Pct exists\n",
    "    if 'ATR_Pct' not in df.columns:\n",
    "        if 'ATR' in df.columns:\n",
    "            atr_pct = df['ATR'] / (df['Close'] + 1e-12)\n",
    "        else:\n",
    "            # compute ATR with window from cfg (fallback to 14)\n",
    "            win = cfg.get('ATR_WINDOW', 14)\n",
    "            atr_series = (\n",
    "                df.groupby(level=1)\n",
    "                  .apply(lambda g: atr(g['High'], g['Low'], g['Close'], win))\n",
    "                  .droplevel(0)\n",
    "            )\n",
    "            atr_pct = atr_series / (df['Close'] + 1e-12)\n",
    "        df = df.assign(ATR_Pct=atr_pct)\n",
    "\n",
    "    # k-day forward log return target\n",
    "    close = df['Close']\n",
    "    fwd_close = close.groupby(level=1).shift(-cfg['RET_HORIZON'])\n",
    "    y_ret = np.log(fwd_close / (close + 1e-12)).rename('y_ret')\n",
    "\n",
    "    tp_mult, sl_mult, timeout = cfg['TB_ATR_TP'], cfg['TB_ATR_SL'], cfg['TB_TIMEOUT']\n",
    "\n",
    "    def per_symbol(g):\n",
    "        res = pd.Series(index=g.index, dtype=float)\n",
    "        atrp = g['ATR_Pct']\n",
    "        # iterate forward to check hits within timeout window\n",
    "        for i, (dt, row) in enumerate(g.iterrows()):\n",
    "            entry = row['Close']\n",
    "            atr = atrp.iloc[i]\n",
    "            if pd.isna(entry) or pd.isna(atr):\n",
    "                res.iloc[i] = np.nan\n",
    "                continue\n",
    "\n",
    "            tp = entry * (1.0 + tp_mult * atr)\n",
    "            sl = entry * (1.0 - sl_mult * atr)\n",
    "\n",
    "            fut = g.iloc[i+1 : i+1+timeout]\n",
    "            outcome = np.nan\n",
    "            for _, r in fut.iterrows():\n",
    "                if r['Low'] <= sl:\n",
    "                    outcome = 0.0\n",
    "                    break\n",
    "                if r['High'] >= tp:\n",
    "                    outcome = 1.0\n",
    "                    break\n",
    "\n",
    "            if np.isnan(outcome):\n",
    "                if len(fut) == 0:\n",
    "                    outcome = np.nan\n",
    "                else:\n",
    "                    outcome = 1.0 if fut.iloc[-1]['Close'] > entry else 0.0\n",
    "\n",
    "            res.iloc[i] = outcome\n",
    "        return res\n",
    "\n",
    "    # BUGFIX: use df here (not outer 'features')\n",
    "    y_tb = df.groupby(level=1, group_keys=False).apply(per_symbol).rename('y_meta')\n",
    "    return y_tb, y_ret\n",
    "\n",
    "# Re-run labels\n",
    "y_meta, y_ret = triple_barrier_labels(features, CONFIG)\n",
    "data = features.join([y_meta, y_ret], how='left').dropna().copy()\n",
    "print('Labeled rows:', len(data))\n",
    "display(data[['y_meta','y_ret']].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7588aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based splits & scaling\n",
    "def time_split(df_index, val_start, test_start):\n",
    "    dates = df_index.get_level_values(0)\n",
    "    train_idx = dates < pd.to_datetime(val_start)\n",
    "    val_idx   = (dates >= pd.to_datetime(val_start)) & (dates < pd.to_datetime(test_start))\n",
    "    test_idx  = dates >= pd.to_datetime(test_start)\n",
    "    return train_idx, val_idx, test_idx\n",
    "\n",
    "train_m, val_m, test_m = time_split(data.index, CONFIG['VAL_START'], CONFIG['TEST_START'])\n",
    "\n",
    "Y_cols = ['y_meta','y_ret']\n",
    "X_cols = [c for c in data.columns if c not in Y_cols]\n",
    "X = data[X_cols].astype(float)\n",
    "Ym = data['y_meta'].astype(int)\n",
    "Yr = data['y_ret'].astype(float)\n",
    "\n",
    "X_train, Ym_train, Yr_train = X[train_m], Ym[train_m], Yr[train_m]\n",
    "X_val,   Ym_val,   Yr_val   = X[val_m],   Ym[val_m],   Yr[val_m]\n",
    "X_test,  Ym_test,  Yr_test  = X[test_m],  Ym[test_m],  Yr[test_m]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_val_s   = scaler.transform(X_val)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "print('Shapes:')\n",
    "print('Train:', X_train_s.shape, 'Val:', X_val_s.shape, 'Test:', X_test_s.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dfc8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence Dataset (sliding windows over each symbol)\n",
    "from typing import List, Tuple\n",
    "\n",
    "def build_panel_arrays(X_df, Ym_s, Yr_s, seq_len):\n",
    "    # Returns dict per symbol: {'X': np.array[T,F], 'Ym': np.array[T], 'Yr': np.array[T]}\n",
    "    panel = {}\n",
    "    for sym, gX in X_df.groupby(level=1):\n",
    "        gX = gX.droplevel(1)\n",
    "        idx = gX.index\n",
    "        gYm = Ym_s.loc[(idx, sym)].values if isinstance(Ym_s.index, pd.MultiIndex) else Ym_s.loc[idx].values\n",
    "        gYr = Yr_s.loc[(idx, sym)].values if isinstance(Yr_s.index, pd.MultiIndex) else Yr_s.loc[idx].values\n",
    "        panel[sym] = {'X': gX.values, 'Ym': gYm, 'Yr': gYr, 'dates': idx}\n",
    "    return panel\n",
    "\n",
    "def make_sequences(panel, seq_len):\n",
    "    seqs = []\n",
    "    for sym, d in panel.items():\n",
    "        Xv, Ymv, Yrv, dates = d['X'], d['Ym'], d['Yr'], d['dates']\n",
    "        T = len(Xv)\n",
    "        if T <= seq_len: continue\n",
    "        for t0 in range(0, T - seq_len):\n",
    "            t1 = t0 + seq_len\n",
    "            x = Xv[t0:t1]\n",
    "            ym = Ymv[t1-1]   # predict at the last step\n",
    "            yr = Yrv[t1-1]\n",
    "            dt = dates[t1-1]\n",
    "            seqs.append((sym, dt, x, ym, yr))\n",
    "    return seqs\n",
    "\n",
    "train_panel = build_panel_arrays(X.loc[train_m], Ym, Yr, CONFIG['SEQ_LEN'])\n",
    "val_panel   = build_panel_arrays(X.loc[val_m], Ym, Yr, CONFIG['SEQ_LEN'])\n",
    "test_panel  = build_panel_arrays(X.loc[test_m], Ym, Yr, CONFIG['SEQ_LEN'])\n",
    "\n",
    "train_seqs = make_sequences(train_panel, CONFIG['SEQ_LEN'])\n",
    "val_seqs   = make_sequences(val_panel,   CONFIG['SEQ_LEN'])\n",
    "test_seqs  = make_sequences(test_panel,  CONFIG['SEQ_LEN'])\n",
    "\n",
    "print('Num sequences:', len(train_seqs), len(val_seqs), len(test_seqs))\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, seqs, scaler: StandardScaler):\n",
    "        self.seqs = seqs\n",
    "        self.scaler = scaler\n",
    "    def __len__(self): return len(self.seqs)\n",
    "    def __getitem__(self, i):\n",
    "        sym, dt, x, ym, yr = self.seqs[i]\n",
    "        x_s = self.scaler.transform(x) if x.ndim==2 else x  # (L,F)\n",
    "        return torch.tensor(x_s, dtype=torch.float32), torch.tensor([ym], dtype=torch.float32), torch.tensor([yr], dtype=torch.float32), sym, dt\n",
    "\n",
    "train_ds = SeqDataset(train_seqs, scaler)\n",
    "val_ds   = SeqDataset(val_seqs,   scaler)\n",
    "test_ds  = SeqDataset(test_seqs,  scaler)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_ds, batch_size=CONFIG['BATCH_SIZE'], shuffle=True, num_workers=4, pin_memory=(device.type!='cpu'), persistent_workers=False)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=CONFIG['BATCH_SIZE'], shuffle=False, num_workers=2, pin_memory=(device.type!='cpu'))\n",
    "test_loader  = DataLoader(test_ds,  batch_size=CONFIG['BATCH_SIZE'], shuffle=False, num_workers=2, pin_memory=(device.type!='cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49b472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models: PatchTST & TCN (multi-task heads)\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as cp\n",
    "import torch\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, p=0.1, ff_mult=4):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=p, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_mult*d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p),\n",
    "            nn.Linear(ff_mult*d_model, d_model),\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.drop = nn.Dropout(p)\n",
    "    def forward(self, x):\n",
    "        def attn_forward(x):\n",
    "            out,_ = self.attn(x, x, x, need_weights=False)\n",
    "            return out\n",
    "        a = cp.checkpoint(attn_forward, x)\n",
    "        x = self.ln1(x + self.drop(a))\n",
    "        f = self.ff(x)\n",
    "        x = self.ln2(x + self.drop(f))\n",
    "        return x\n",
    "\n",
    "class PatchTST(nn.Module):\n",
    "    def __init__(self, in_dim, cfg):\n",
    "        super().__init__()\n",
    "        self.patch_len = cfg['PATCH_LEN']; self.stride = cfg['PATCH_STRIDE']\n",
    "        self.d_model = cfg['DMODEL']; self.n_layers = cfg['N_LAYERS']; self.n_heads = cfg['N_HEADS']\n",
    "        self.proj = nn.Linear(in_dim*self.patch_len, self.d_model)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(self.d_model, self.n_heads, p=cfg['DROPOUT']) for _ in range(self.n_layers)])\n",
    "        self.norm = nn.LayerNorm(self.d_model)\n",
    "        self.head_meta = nn.Sequential(nn.Linear(self.d_model, 1))\n",
    "        self.head_ret  = nn.Sequential(nn.Linear(self.d_model, 1))\n",
    "        self.recon = nn.Linear(self.d_model, in_dim*self.patch_len)\n",
    "\n",
    "    def patchify(self, x):  # x: (B, L, F)\n",
    "        B,L,F = x.shape\n",
    "        n = 1 + (L - self.patch_len)//self.stride\n",
    "        patches = []\n",
    "        for i in range(n):\n",
    "            s = i*self.stride\n",
    "            patches.append(x[:, s:s+self.patch_len, :].reshape(B, -1))\n",
    "        return torch.stack(patches, dim=1)  # (B, n_patches, F*patch_len)\n",
    "\n",
    "    def forward(self, x, pretrain_mask=None):\n",
    "        z = self.patchify(x)\n",
    "        z = self.proj(z)\n",
    "        for blk in self.blocks:\n",
    "            z = blk(z)\n",
    "        h = self.norm(z)[:, -1]\n",
    "        meta_logit = self.head_meta(h).squeeze(-1)\n",
    "        ret = self.head_ret(h).squeeze(-1)\n",
    "\n",
    "        recon_loss = None\n",
    "        if pretrain_mask is not None:\n",
    "            B, N, D = z.shape\n",
    "            masked_tokens = z[pretrain_mask]\n",
    "            pred_vec = self.recon(masked_tokens)\n",
    "            patches = self.patchify(x).detach()\n",
    "            target_vec = patches[pretrain_mask]\n",
    "            recon_loss = nn.functional.mse_loss(pred_vec, target_vec)\n",
    "        return meta_logit, ret, recon_loss\n",
    "\n",
    "class TCNBlock(nn.Module):\n",
    "    def __init__(self, c_in, c_out, k=7, d=1, p=0.1):\n",
    "        super().__init__()\n",
    "        pad = (k-1)*d\n",
    "        self.net = nn.Sequential(\n",
    "            nn.utils.weight_norm(nn.Conv1d(c_in, c_out, k, padding=pad, dilation=d)),\n",
    "            nn.ReLU(), nn.Dropout(p),\n",
    "            nn.utils.weight_norm(nn.Conv1d(c_out, c_out, k, padding=pad, dilation=d)),\n",
    "            nn.ReLU(), nn.Dropout(p),\n",
    "        )\n",
    "        self.res = nn.Conv1d(c_in, c_out, 1) if c_in!=c_out else nn.Identity()\n",
    "    def forward(self, x):\n",
    "        def fn(inp): return self.net(inp)\n",
    "        out = cp.checkpoint(fn, x)\n",
    "        if out.size(-1) != x.size(-1):\n",
    "            out = out[..., :x.size(-1)]\n",
    "        return out + self.res(x)\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, in_dim, cfg):\n",
    "        super().__init__()\n",
    "        chans = cfg['TCN_CHANNELS']; k = cfg['TCN_KERNEL']; p = cfg['TCN_DROPOUT']\n",
    "        layers=[]; c=in_dim\n",
    "        for i,ch in enumerate(chans):\n",
    "            layers += [TCNBlock(c, ch, k=k, d=2**i, p=p)]\n",
    "            c = ch\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.head_meta = nn.Linear(c, 1)\n",
    "        self.head_ret  = nn.Linear(c, 1)\n",
    "    def forward(self, x, pretrain_mask=None):\n",
    "        z = x.transpose(1,2)\n",
    "        z = self.tcn(z)\n",
    "        h = z[:,:,-1]\n",
    "        meta_logit = self.head_meta(h).squeeze(-1)\n",
    "        ret = self.head_ret(h).squeeze(-1)\n",
    "        return meta_logit, ret, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a45520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Validate with AMP, checkpointing, EMA, early stopping\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "from torch.optim.swa_utils import AveragedModel\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def make_model(in_dim, cfg):\n",
    "    if cfg['MODEL_TYPE']=='patchtst':\n",
    "        model = PatchTST(in_dim, cfg)\n",
    "    else:\n",
    "        model = TCN(in_dim, cfg)\n",
    "    model = model.to(device)\n",
    "    try:\n",
    "        model = torch.compile(model)\n",
    "        print('torch.compile enabled')\n",
    "    except Exception as e:\n",
    "        print('torch.compile skipped:', e)\n",
    "    return model\n",
    "\n",
    "def run_epoch(model, loader, optimizers=None, cfg=None, pretrain=False):\n",
    "    is_train = optimizers is not None\n",
    "    model.train(mode=is_train)\n",
    "\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    huber = nn.HuberLoss(delta=0.01)\n",
    "    total_loss = 0.0\n",
    "    all_meta, all_meta_p = [], []\n",
    "    all_ret, all_ret_p = [], []\n",
    "\n",
    "    for step, (xb, ymb, yrb, _, _) in enumerate(loader):\n",
    "        xb = xb.to(device)    # (B, L, F)\n",
    "        ymb = ymb.to(device).squeeze(-1).float()\n",
    "        yrb = yrb.to(device).squeeze(-1).float()\n",
    "\n",
    "        pre_mask = None\n",
    "        if pretrain and isinstance(model, PatchTST):\n",
    "            with torch.no_grad():\n",
    "                B,L,F = xb.shape\n",
    "                N = 1 + (L - CONFIG['PATCH_LEN'])//CONFIG['PATCH_STRIDE']\n",
    "                mask = torch.rand((B,N), device=device) < CONFIG['MASK_PROB']\n",
    "                pre_mask = mask\n",
    "\n",
    "        with torch.autocast(device_type=device.type, dtype=torch.float16, enabled=(device.type in ['cuda','mps'])):\n",
    "            meta_logit, ret_pred, recon_loss = model(xb, pretrain_mask=(pre_mask if pretrain else None))\n",
    "            if pretrain and recon_loss is not None:\n",
    "                loss = recon_loss\n",
    "            else:\n",
    "                loss_meta = bce(meta_logit, ymb)\n",
    "                loss_ret  = huber(ret_pred, yrb)\n",
    "                loss = CONFIG['LOSS_META_W']*loss_meta + CONFIG['LOSS_RET_W']*loss_ret\n",
    "\n",
    "        if is_train:\n",
    "            optimizers['opt'].zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), CONFIG['GRAD_CLIP'])\n",
    "            optimizers['opt'].step()\n",
    "            if 'sched' in optimizers and optimizers['sched'] is not None:\n",
    "                optimizers['sched'].step()\n",
    "\n",
    "        total_loss += loss.item()*xb.size(0)\n",
    "\n",
    "        if not pretrain:\n",
    "            all_meta.append(ymb.detach().cpu().numpy())\n",
    "            all_meta_p.append(torch.sigmoid(meta_logit).detach().cpu().numpy())\n",
    "            all_ret.append(yrb.detach().cpu().numpy())\n",
    "            all_ret_p.append(ret_pred.detach().cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    out = {'loss': avg_loss}\n",
    "    if not pretrain:\n",
    "        ym = np.concatenate(all_meta) if len(all_meta)>0 else np.array([])\n",
    "        yp = np.concatenate(all_meta_p) if len(all_meta_p)>0 else np.array([])\n",
    "        yr = np.concatenate(all_ret) if len(all_ret)>0 else np.array([])\n",
    "        yrp= np.concatenate(all_ret_p) if len(all_ret_p)>0 else np.array([])\n",
    "        if len(ym)>0 and len(np.unique(ym))==2:\n",
    "            try: auc = roc_auc_score(ym, yp)\n",
    "            except: auc = float('nan')\n",
    "        else:\n",
    "            auc = float('nan')\n",
    "        mae = mean_absolute_error(yr, yrp) if len(yr)>0 else float('nan')\n",
    "        rmse = (mean_squared_error(yr, yrp)**0.5) if len(yr)>0 else float('nan')\n",
    "        out.update({'AUC': auc, 'MAE': mae, 'RMSE': rmse})\n",
    "    return out\n",
    "\n",
    "in_dim = X_train.shape[1]\n",
    "model = make_model(in_dim, CONFIG)\n",
    "opt = optim.AdamW(model.parameters(), lr=CONFIG['LR'], weight_decay=CONFIG['WEIGHT_DECAY'])\n",
    "sched = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=CONFIG['EPOCHS'])\n",
    "ema = AveragedModel(model)\n",
    "\n",
    "# Optional pretraining\n",
    "if CONFIG['DO_PRETRAIN'] and isinstance(model, PatchTST):\n",
    "    print('Pretraining (masked patches):', CONFIG['PRE_EPOCHS'], 'epochs')\n",
    "    for ep in range(1, CONFIG['PRE_EPOCHS']+1):\n",
    "        tr = run_epoch(model, train_loader, optimizers={'opt':opt, 'sched':None}, cfg=CONFIG, pretrain=True)\n",
    "        vl = run_epoch(model, val_loader, optimizers=None, cfg=CONFIG, pretrain=True)\n",
    "        print(f'[PRE] Ep {ep:02d} | train_loss={tr[\"loss\"]:.4f} | val_loss={vl[\"loss\"]:.4f}')\n",
    "\n",
    "best_val, best_state, patience = -np.inf, None, CONFIG['PATIENCE']\n",
    "history = defaultdict(list)\n",
    "\n",
    "# Supervised fine-tune\n",
    "for ep in range(1, CONFIG['EPOCHS']+1):\n",
    "    tr = run_epoch(model, train_loader, optimizers={'opt':opt, 'sched':sched}, cfg=CONFIG, pretrain=False)\n",
    "    vl = run_epoch(model, val_loader, optimizers=None, cfg=CONFIG, pretrain=False)\n",
    "    ema.update_parameters(model)\n",
    "    history['train_loss'].append(tr['loss']); history['val_loss'].append(vl['loss'])\n",
    "    history['train_auc'].append(tr.get('AUC', np.nan)); history['val_auc'].append(vl.get('AUC', np.nan))\n",
    "    print(f'Ep {ep:02d} | train_loss={tr[\"loss\"]:.4f} AUC={tr.get(\"AUC\",np.nan):.4f} | val_loss={vl[\"loss\"]:.4f} AUC={vl.get(\"AUC\",np.nan):.4f}')\n",
    "    score = vl.get('AUC', -np.inf)\n",
    "    if score > best_val + 1e-4:\n",
    "        best_val = score; best_state = model.state_dict(); patience = CONFIG['PATIENCE']\n",
    "        torch.save(best_state, os.path.join(SAVE_DIR, 'best_pro.pt'))\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience <= 0:\n",
    "            print('Early stopping.'); break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641f2919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(); plt.plot(history['train_loss'], label='train'); plt.plot(history['val_loss'], label='val'); plt.title('Loss'); plt.legend(); plt.show()\n",
    "plt.figure(); plt.plot(history['train_auc'], label='train'); plt.plot(history['val_auc'], label='val'); plt.title('AUC'); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f14bcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluation\n",
    "def predict_loader(model, loader):\n",
    "    model.eval()\n",
    "    all_meta_p, all_ret_p, metas, rets, idx = [], [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, ymb, yrb, sym, dt in loader:\n",
    "            xb = xb.to(device)\n",
    "            ml, rp, _ = model(xb, pretrain_mask=None)\n",
    "            mp = torch.sigmoid(ml).cpu().numpy()\n",
    "            all_meta_p.append(mp); all_ret_p.append(rp.cpu().numpy())\n",
    "            metas.append(ymb.numpy().squeeze()); rets.append(yrb.numpy().squeeze())\n",
    "            for s,d in zip(sym, dt):\n",
    "                idx.append((pd.Timestamp(d), s))\n",
    "    meta_p = np.concatenate(all_meta_p); ret_p = np.concatenate(all_ret_p)\n",
    "    meta = np.concatenate(metas); ret = np.concatenate(rets)\n",
    "    idx = pd.MultiIndex.from_tuples(idx, names=['Date','Symbol'])\n",
    "    return pd.DataFrame({'p_trade': meta_p, 'y_meta': meta, 'y_ret': ret, 'ret_pred': ret_p}, index=idx)\n",
    "\n",
    "df_test_pred = predict_loader(model, test_loader)\n",
    "try:\n",
    "    auc_test = roc_auc_score(df_test_pred['y_meta'], df_test_pred['p_trade'])\n",
    "except Exception:\n",
    "    auc_test = float('nan')\n",
    "mae_test = mean_absolute_error(df_test_pred['y_ret'], df_test_pred['ret_pred'])\n",
    "rmse_test = mean_squared_error(df_test_pred['y_ret'], df_test_pred['ret_pred'])**0.5\n",
    "print({'AUC_test': round(auc_test,4), 'MAE_test': round(mae_test,6), 'RMSE_test': round(rmse_test,6)})\n",
    "display(df_test_pred.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41c36af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest (uses p_trade and ATR TP/SL)\n",
    "def daily_vol(series, window=10):\n",
    "    return series.pct_change().rolling(window).std()\n",
    "\n",
    "def select_positions(prob_df, date, top_k=CONFIG['TOP_K'], cutoff=CONFIG['PROB_CUTOFF']):\n",
    "    if date not in prob_df.index.get_level_values(0): return []\n",
    "    day = prob_df.xs(date, level=0, drop_level=False)\n",
    "    day = day[day['p_trade'] >= cutoff]\n",
    "    day = day.sort_values('p_trade', ascending=False).head(top_k)\n",
    "    return list(day.index.get_level_values(1))\n",
    "\n",
    "def simulate_trade_path(df, symbol, start_date):\n",
    "    hold_max = CONFIG['HOLD_MAX_DAYS']; tp_mult=CONFIG['ATR_MULT_TP']; sl_mult=CONFIG['ATR_MULT_STOP']\n",
    "    slippage_bps = CONFIG['SLIPPAGE_BPS']; fee_bps = CONFIG['FEE_BPS_ROUNDTRIP']\n",
    "    try:\n",
    "        sub = df.xs(symbol, level=1)\n",
    "    except Exception:\n",
    "        return 0.0, start_date\n",
    "    if start_date not in sub.index:\n",
    "        return 0.0, start_date\n",
    "    entry_px = sub.loc[start_date,'Close']; atr_pct = sub.loc[start_date,'ATR_Pct']\n",
    "    if np.isnan(entry_px) or np.isnan(atr_pct):\n",
    "        return 0.0, start_date\n",
    "    tp = entry_px*(1+tp_mult*atr_pct); sl = entry_px*(1-sl_mult*atr_pct)\n",
    "    fut = sub.loc[start_date:].iloc[1:1+hold_max]\n",
    "    exit_date = start_date; pnl = None\n",
    "    for idx,row in fut.iterrows():\n",
    "        exit_date = idx\n",
    "        if row['Low'] <= sl: pnl = (sl/entry_px - 1.0); break\n",
    "        if row['High'] >= tp: pnl = (tp/entry_px - 1.0); break\n",
    "    if pnl is None:\n",
    "        if len(fut)==0: return 0.0, start_date\n",
    "        pnl = (fut.iloc[-1]['Close']/entry_px - 1.0)\n",
    "    cost = (2*slippage_bps + fee_bps)/10_000.0\n",
    "    return pnl - cost, exit_date\n",
    "\n",
    "def backtest(df_feats, preds):\n",
    "    dates = sorted(preds.index.get_level_values(0).unique())\n",
    "    equity = CONFIG['START_EQUITY']\n",
    "    daily_pnl = pd.Series(0.0, index=dates)\n",
    "    from collections import deque\n",
    "    pause_days = 0; weekly_returns = deque(maxlen=5); last_week = None\n",
    "\n",
    "    spy_close = df_feats.xs('SPY', level=1)['Close']\n",
    "    spy_ma50  = spy_close.rolling(50).mean()\n",
    "    spy_vol10 = daily_vol(spy_close, 10)\n",
    "\n",
    "    import math\n",
    "    for d in dates:\n",
    "        if last_week is None or d.weekday()==0:\n",
    "            if len(weekly_returns)==5 and sum(weekly_returns) <= CONFIG['WEEKLY_LOSS_CAP']:\n",
    "                pause_days = CONFIG['PAUSE_DAYS_ON_WEEKLY_BREACH']\n",
    "            weekly_returns.clear(); last_week = d\n",
    "\n",
    "        day_ret = 0.0\n",
    "        if pause_days>0:\n",
    "            pause_days -= 1\n",
    "            daily_pnl.loc[d] = 0.0\n",
    "            continue\n",
    "\n",
    "        vol_throttle = 1.0\n",
    "        if d in spy_close.index:\n",
    "            bear = spy_close.loc[d] < spy_ma50.loc[d] if not math.isnan(spy_ma50.loc[d]) else False\n",
    "            hist = spy_vol10.loc[:d].dropna()\n",
    "            high_vol = (len(hist)>20) and (hist.iloc[-1] >= hist.quantile(0.9))\n",
    "            if bear and high_vol: vol_throttle = 0.5\n",
    "\n",
    "        picks = select_positions(preds, d, top_k=int(CONFIG['TOP_K']*vol_throttle))\n",
    "        if len(picks)>0:\n",
    "            gross_target = min(1.0, CONFIG['MAX_GROSS_LEV'])*vol_throttle\n",
    "            w = min(1.0/len(picks), CONFIG['PER_NAME_CAP'], gross_target/len(picks))\n",
    "            for sym in picks:\n",
    "                pnl,_ = simulate_trade_path(df_feats, sym, d)\n",
    "                day_ret += w * pnl\n",
    "\n",
    "        if day_ret <= CONFIG['DAILY_LOSS_CAP']:\n",
    "            day_ret = CONFIG['DAILY_LOSS_CAP']; pause_days = 1\n",
    "\n",
    "        equity *= (1.0 + day_ret)\n",
    "        daily_pnl.loc[d] = day_ret; weekly_returns.append(day_ret)\n",
    "\n",
    "    curve = pd.Series(np.cumprod(1+daily_pnl.values)*CONFIG['START_EQUITY'], index=daily_pnl.index, name='equity')\n",
    "    return curve, daily_pnl\n",
    "\n",
    "curve, dret = backtest(data, df_test_pred[['p_trade']])\n",
    "def kpis(curve, dret):\n",
    "    rets = dret.values\n",
    "    sharpe = (rets.mean()/(rets.std()+1e-12))*np.sqrt(252)\n",
    "    sortino = (rets.mean()*np.sqrt(252))/ (np.std(np.minimum(0,rets))*np.sqrt(252)+1e-12)\n",
    "    roll_max = curve.cummax(); maxdd = (curve/roll_max - 1.0).min()\n",
    "    wins = rets[rets>0].sum(); losses = -rets[rets<0].sum()\n",
    "    pf = wins/(losses+1e-12)\n",
    "    return {'Sharpe':sharpe, 'Sortino':sortino, 'MaxDD':maxdd, 'ProfitFactor':pf}\n",
    "\n",
    "k = kpis(curve, dret)\n",
    "print({k_: round(v,3) for k_,v in k.items()})\n",
    "plt.figure(figsize=(10,4)); plt.plot(curve/curve.iloc[0]); plt.title('Equity Curve (Test)'); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc86d7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save artifacts\n",
    "torch.save(model.state_dict(), os.path.join(SAVE_DIR, 'best_pro.pt'))\n",
    "with open(os.path.join(SAVE_DIR, 'config.json'),'w') as f: json.dump(CONFIG, f, indent=2)\n",
    "with open(os.path.join(SAVE_DIR, 'feature_columns.json'),'w') as f: json.dump(X_cols, f, indent=2)\n",
    "import joblib\n",
    "joblib.dump(scaler, os.path.join(SAVE_DIR, 'scaler.joblib'))\n",
    "print('Saved artifacts to', SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6606a202",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b72f24-e4c0-4901-8c95-1d226b81871c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (Anaconda 2024.10)",
   "language": "python",
   "name": "anaconda-2024.10-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
