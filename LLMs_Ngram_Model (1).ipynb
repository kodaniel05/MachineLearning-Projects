{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b172cfd2",
   "metadata": {},
   "source": [
    "\n",
    "# LLMs: N-gram Model (Python)\n",
    "\n",
    "**Dataset:** `/anvil/projects/tdm/data/amazon/music.txt` (~2GB).  \n",
    "\n",
    "\n",
    "**What is built build**\n",
    "- Robust `read_lines` (streamed) and enhanced `clean_text` (lowercasing, escape decoding, alpha-only)\n",
    "- A reusable `NGram` class with:\n",
    "  - `set_data`\n",
    "  - `generate_ngrams`\n",
    "  - `generate_ngram_probabilities`\n",
    "  - `get_next_word` (random/common/uncommon)\n",
    "  - `get_perplexity`\n",
    "- Test calls mirroring the project prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e64cb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import codecs\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Union\n",
    "\n",
    "DATA_PATH = Path('/anvil/projects/tdm/data/amazon/music.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b0af7d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Question 1 — set_data with helpers\n",
    "\n",
    "**Deliverables**\n",
    "- Modified `clean_text` (lowercase + alpha-only)\n",
    "- Implemented `set_data`\n",
    "- Passes provided tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a44f6b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-based set_data test:\n",
      "['hello']\n",
      "\n",
      "Word-based set_data test (first 2 lines):\n",
      "[('i', 'love', 'this', 'cd', 'so', 'inspiring'), ('love', 'it', 'great', 'seller')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_lines(file_path: Union[str, Path], n: int, start: int = 0) -> List[str]:\n",
    "    \"\"\"Stream n lines starting at 0-based index 'start'.\"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    out = []\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i < start:\n",
    "                continue\n",
    "            if i >= start + n:\n",
    "                break\n",
    "            out.append(line)\n",
    "    return out\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Decode escapes, strip surrounding quotes, normalize whitespace,\n",
    "    lowercase, and keep only alphabet + spaces.\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        decoded = codecs.decode(text, 'unicode_escape')\n",
    "    except Exception:\n",
    "        decoded = text\n",
    "    s = decoded.strip()\n",
    "    # Remove starting/ending quotes if both present\n",
    "    if len(s) >= 2 and s[0] == '\"' and s[-1] == '\"':\n",
    "        s = s[1:-1]\n",
    "    # Replace actual control chars with space\n",
    "    s = s.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "    # Lowercase\n",
    "    s = s.lower()\n",
    "    # Keep only letters and spaces\n",
    "    s = ''.join(c for c in s if c.isalpha() or c.isspace())\n",
    "    # Normalize multiple spaces\n",
    "    s = ' '.join(s.split())\n",
    "    return s\n",
    "\n",
    "class NGram:\n",
    "    def __init__(self, n: int, is_character_based: bool = False):\n",
    "        self.n = int(n)\n",
    "        self.is_character_based = bool(is_character_based)\n",
    "        self.data = None  # list[str] for character mode; list[tuple[str,...]] for word mode\n",
    "        self.ngrams = set()\n",
    "        self.ngram_frequencies: Dict[Union[str, Tuple[str,...]], int] = {}\n",
    "        # Probabilities: context (len n-1) -> { next_token: prob }\n",
    "        self.ngram_probabilities: Dict[Union[str, Tuple[str,...]], Dict[str, float]] = {}\n",
    "\n",
    "    def set_data(self, data: List[str]):\n",
    "        \"\"\"Ingest raw lines; clean; then:\n",
    "        - character-based: self.data = list[str] (one string per line)\n",
    "        - word-based: self.data = list[tuple[str,...]] (hashable sequences)\"\"\"\n",
    "        if not isinstance(data, list):\n",
    "            return\n",
    "        cleaned = [clean_text(s) for s in data if isinstance(s, str)]\n",
    "        if self.is_character_based:\n",
    "            # Keep as list of strings (characters will be slid over these strings)\n",
    "            self.data = [s for s in cleaned if s]  # drop empties\n",
    "        else:\n",
    "            # Split into tuples-of-words for hashability\n",
    "            sequences = []\n",
    "            for s in cleaned:\n",
    "                if not s:\n",
    "                    continue\n",
    "                toks = s.split()\n",
    "                if toks:\n",
    "                    sequences.append(tuple(toks))\n",
    "            self.data = sequences\n",
    "\n",
    "# Tests from prompt for set_data\n",
    "print(\"Character-based set_data test:\")\n",
    "my_ngram = NGram(2, is_character_based=True)\n",
    "my_ngram.set_data(['\"hello!\"'])\n",
    "print(my_ngram.data)  # expect ['hello']\n",
    "\n",
    "print(\"\\nWord-based set_data test (first 2 lines):\")\n",
    "my_ngram = NGram(2, is_character_based=False)\n",
    "my_ngram.set_data(read_lines(DATA_PATH, 2))\n",
    "print(my_ngram.data[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06d82c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Question 2 — generate_ngrams\n",
    "\n",
    "Build frequency dictionary for each n-gram:\n",
    "- **Character-based**: slide window of size `n` over each string\n",
    "- **Word-based**: slide window of size `n` over each tuple of tokens\n",
    "\n",
    "Return and store in `self.ngram_frequencies`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6c1c69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate ngrams (char-based) — 1 line at index 14:\n",
      "{'ha': 3, 'ad': 1, 'd ': 3, ' t': 3, 'th': 4, 'hi': 2, 'is': 2, 's ': 5, ' a': 4, 'as': 1, 'an': 1, 'n ': 2, 'al': 2, 'lb': 1, 'bu': 1, 'um': 1, 'm ': 1, ' b': 1, 'ba': 1, 'ac': 1, 'ck': 1, 'k ': 1, ' i': 1, 'in': 1, 'he': 1, 'e ': 4, ' d': 1, 'da': 1, 'ay': 2, 'y ': 2, ' h': 2, 'av': 2, 've': 3, 'lw': 1, 'wa': 1, 'ys': 1, ' e': 2, 'en': 2, 'nj': 1, 'jo': 1, 'oy': 1, 'ye': 1, 'ed': 1, ' k': 1, 'ki': 1, 'ie': 1, 'et': 1, 'h ': 1, ' g': 1, 'gr': 1, 're': 1, 'ee': 1, 'ns': 1, ' m': 1, 'mu': 1, 'us': 1, 'si': 1, 'ic': 1, 'c ': 1, 'ev': 1, 'er': 1, 'ry': 1, ' o': 1, 'on': 1, 'ne': 1, ' s': 1, 'sh': 1, 'ho': 1, 'ou': 1, 'ul': 1, 'ld': 1, ' c': 1, 'cd': 1}\n",
      "\n",
      "Generate ngrams (word-based) — 1 line at index 14:\n",
      "{('had', 'this'): 1, ('this', 'as'): 1, ('as', 'an'): 1, ('an', 'album'): 1, ('album', 'back'): 1, ('back', 'in'): 1, ('in', 'the'): 1, ('the', 'day'): 1, ('day', 'have'): 1, ('have', 'always'): 1, ('always', 'enjoyed'): 1, ('enjoyed', 'kieth'): 1, ('kieth', 'greens'): 1, ('greens', 'music'): 1, ('music', 'every'): 1, ('every', 'one'): 1, ('one', 'should'): 1, ('should', 'have'): 1, ('have', 'this'): 1, ('this', 'cd'): 1}\n"
     ]
    }
   ],
   "source": [
    "def _windows(seq, n):\n",
    "    for i in range(len(seq) - n + 1):\n",
    "        yield seq[i:i+n]\n",
    "\n",
    "def _add_freq(freq: dict, key):\n",
    "    freq[key] = freq.get(key, 0) + 1\n",
    "\n",
    "def generate_ngrams(self) -> Dict[Union[str, Tuple[str,...]], int]:\n",
    "    if not isinstance(self.data, list) or self.n <= 0:\n",
    "        self.ngram_frequencies = {}\n",
    "        return {}\n",
    "    freq = {}\n",
    "    if self.is_character_based:\n",
    "        for s in self.data:\n",
    "            if len(s) < self.n:\n",
    "                continue\n",
    "            for win in _windows(s, self.n):\n",
    "                _add_freq(freq, win)  # win is a string slice\n",
    "    else:\n",
    "        for tup in self.data:\n",
    "            if len(tup) < self.n:\n",
    "                continue\n",
    "            for win in _windows(tup, self.n):\n",
    "                _add_freq(freq, tuple(win))  # ensure tuple key\n",
    "    self.ngram_frequencies = freq\n",
    "    self.ngrams = set(freq.keys())\n",
    "    return freq\n",
    "\n",
    "# Bind to class\n",
    "setattr(NGram, \"generate_ngrams\", generate_ngrams)\n",
    "\n",
    "# Tests (character & word for a specific line index)\n",
    "print(\"Generate ngrams (char-based) — 1 line at index 14:\")\n",
    "my_ngram = NGram(2, is_character_based=True)\n",
    "my_ngram.set_data(read_lines(DATA_PATH, 1, 14))\n",
    "print(my_ngram.generate_ngrams())\n",
    "\n",
    "print(\"\\nGenerate ngrams (word-based) — 1 line at index 14:\")\n",
    "my_ngram = NGram(2, is_character_based=False)\n",
    "my_ngram.set_data(read_lines(DATA_PATH, 1, 14))\n",
    "print(my_ngram.generate_ngrams())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0d25fb",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Question 3 — generate_ngram_probabilities\n",
    "\n",
    "Create nested mapping: **context (n−1)** → { **next**: probability } using frequency counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caa36d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_ngram_probabilities — 3-gram word-based on 5 lines starting at 3:\n",
      "('as', 'good') -> {'as': 1.0}\n",
      "('good', 'as') -> {'i': 1.0}\n",
      "('as', 'i') -> {'remember': 1.0}\n",
      "('i', 'remember') -> {'back': 1.0}\n",
      "('remember', 'back') -> {'when': 1.0}\n",
      "('back', 'when') -> {'i': 1.0}\n",
      "('when', 'i') -> {'bought': 1.0}\n",
      "('i', 'bought') -> {'the': 1.0}\n",
      "('bought', 'the') -> {'one': 1.0}\n",
      "('the', 'one') -> {'in': 1.0}\n"
     ]
    }
   ],
   "source": [
    "def generate_ngram_probabilities(self):\n",
    "    if not isinstance(self.ngram_frequencies, dict) or not self.ngram_frequencies:\n",
    "        # If not yet built, try to build\n",
    "        self.generate_ngrams()\n",
    "        if not self.ngram_frequencies:\n",
    "            self.ngram_probabilities = {}\n",
    "            return {}\n",
    "    probs = {}\n",
    "    for ngram, cnt in self.ngram_frequencies.items():\n",
    "        if self.is_character_based:\n",
    "            context = ngram[:-1]  # string slice\n",
    "            nxt = ngram[-1]\n",
    "        else:\n",
    "            # ngram is a tuple of tokens\n",
    "            context = ngram[:-1]\n",
    "            nxt = ngram[-1]\n",
    "        # Normalize key types: context stays same type; next must be str\n",
    "        nxt_str = nxt if isinstance(nxt, str) else str(nxt)\n",
    "        # Accumulate counts by context\n",
    "        if context not in probs:\n",
    "            probs[context] = {}\n",
    "        probs[context][nxt_str] = probs[context].get(nxt_str, 0) + cnt\n",
    "\n",
    "    # Convert counts to probabilities\n",
    "    for ctx, nxt_dict in probs.items():\n",
    "        total = float(sum(nxt_dict.values()))\n",
    "        for k in list(nxt_dict.keys()):\n",
    "            nxt_dict[k] = nxt_dict[k] / total\n",
    "    self.ngram_probabilities = probs\n",
    "    return probs\n",
    "\n",
    "# Bind to class\n",
    "setattr(NGram, \"generate_ngram_probabilities\", generate_ngram_probabilities)\n",
    "\n",
    "# Test from prompt (3-gram, word-based)\n",
    "print(\"generate_ngram_probabilities — 3-gram word-based on 5 lines starting at 3:\")\n",
    "my_ngram = NGram(3, is_character_based=False)\n",
    "my_ngram.set_data(read_lines(DATA_PATH, 5, 3))\n",
    "probs = my_ngram.generate_ngram_probabilities()\n",
    "# Print a small slice\n",
    "items = list(probs.items())[:10]\n",
    "for k,v in items:\n",
    "    print(k, \"->\", v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955c0274",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Question 4 — get_next_word\n",
    "\n",
    "Given previous (n−1) tokens/chars, return:\n",
    "- **random**: sample via probabilities\n",
    "- **common**: most-probable next\n",
    "- **uncommon**: least-probable next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fd8567b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_next_word tests on 3-gram word-based, 300 lines starting at 50:\n",
      "random #1: truly\n",
      "random #2: very\n",
      "random #3: show\n",
      "common: great\n",
      "uncommon: beautiful\n"
     ]
    }
   ],
   "source": [
    "def _normalize_input_for_context(s: str, is_char: bool, n_minus_1: int):\n",
    "    \"\"\"Return context key (matching how we stored contexts).\"\"\"\n",
    "    s = clean_text(s)  # same cleaning pipeline as training\n",
    "    if is_char:\n",
    "        # Need exactly n-1 characters\n",
    "        if len(s) != n_minus_1:\n",
    "            return None\n",
    "        return s  # string slice used as key\n",
    "    else:\n",
    "        toks = tuple(s.split())\n",
    "        if len(toks) != n_minus_1:\n",
    "            return None\n",
    "        return toks\n",
    "\n",
    "def get_next_word(self, previous_words: str, method: str = 'random') -> str:\n",
    "    if not isinstance(self.ngram_probabilities, dict) or not self.ngram_probabilities:\n",
    "        return \"\"\n",
    "    n_minus_1 = self.n - 1\n",
    "    ctx = _normalize_input_for_context(previous_words, self.is_character_based, n_minus_1)\n",
    "    if ctx is None or ctx not in self.ngram_probabilities:\n",
    "        return \"\"\n",
    "    candidates = list(self.ngram_probabilities[ctx].keys())\n",
    "    probs = np.array([self.ngram_probabilities[ctx][k] for k in candidates], dtype=float)\n",
    "    if method == 'random':\n",
    "        # Safe: normalize in case of tiny rounding issues\n",
    "        probs = probs / probs.sum()\n",
    "        return np.random.choice(candidates, p=probs)\n",
    "    elif method == 'common':\n",
    "        max_p = probs.max()\n",
    "        # Tie-break deterministically by lexicographic order\n",
    "        winners = [c for c, p in zip(candidates, probs) if p == max_p]\n",
    "        return sorted(winners)[0]\n",
    "    elif method == 'uncommon':\n",
    "        min_p = probs.min()\n",
    "        losers = [c for c, p in zip(candidates, probs) if p == min_p]\n",
    "        return sorted(losers)[0]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Bind\n",
    "setattr(NGram, \"get_next_word\", get_next_word)\n",
    "\n",
    "# Tests from prompt (seeded random for reproducibility)\n",
    "print(\"get_next_word tests on 3-gram word-based, 300 lines starting at 50:\")\n",
    "np.random.seed(18)\n",
    "ng = NGram(3, is_character_based=False)\n",
    "ng.set_data(read_lines(DATA_PATH, 300, 50))\n",
    "ng.generate_ngram_probabilities()\n",
    "print(\"random #1:\", ng.get_next_word('is a', method='random'))\n",
    "print(\"random #2:\", ng.get_next_word('is a', method='random'))\n",
    "print(\"random #3:\", ng.get_next_word('is a', method='random'))\n",
    "print(\"common:\", ng.get_next_word('is a', method='common'))\n",
    "print(\"uncommon:\", ng.get_next_word('is a', method='uncommon'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5699227e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Question 5 — get_perplexity\n",
    "\n",
    "Perplexity \\(= \\exp\\left(-\\tfrac{1}{N}\\sum_i \\log p_i\\right)\\)  \n",
    "If an n-gram is unseen, use probability **1e-5** (heavy penalty).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "986f3b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity examples:\n",
      "is a great cd -> 8.658734561731656\n",
      "is a good cd -> 38.683904203744014\n",
      "is a bad cd -> 100000.00000000001\n",
      "this music is a wonderful experience and i love it -> 143.91101930501804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "def get_perplexity(self, text: str) -> float:\n",
    "    if not isinstance(self.ngram_probabilities, dict) or not self.ngram_probabilities:\n",
    "        return float('inf')\n",
    "    text_clean = clean_text(text)\n",
    "    if self.is_character_based:\n",
    "        seq = text_clean  # string\n",
    "        if len(seq) < self.n:\n",
    "            return float('inf')\n",
    "        # iterate over consecutive n-grams\n",
    "        probs = []\n",
    "        for i in range(len(seq) - self.n + 1):\n",
    "            context = seq[i:i+self.n-1]\n",
    "            nxt = seq[i+self.n-1]\n",
    "            ctx = context\n",
    "            if ctx in self.ngram_probabilities and nxt in self.ngram_probabilities[ctx]:\n",
    "                p = self.ngram_probabilities[ctx][nxt]\n",
    "            else:\n",
    "                p = 1e-5\n",
    "            probs.append(p)\n",
    "    else:\n",
    "        toks = tuple(text_clean.split())\n",
    "        if len(toks) < self.n:\n",
    "            return float('inf')\n",
    "        probs = []\n",
    "        for i in range(len(toks) - self.n + 1):\n",
    "            context = toks[i:i+self.n-1]\n",
    "            nxt = toks[i+self.n-1]\n",
    "            if context in self.ngram_probabilities and nxt in self.ngram_probabilities[context]:\n",
    "                p = self.ngram_probabilities[context][nxt]\n",
    "            else:\n",
    "                p = 1e-5\n",
    "            probs.append(p)\n",
    "    if not probs:\n",
    "        return float('inf')\n",
    "    probs = np.array(probs, dtype=float)\n",
    "    # Natural log base (e)\n",
    "    return float(np.exp(-np.mean(np.log(probs))))\n",
    "\n",
    "# Bind\n",
    "setattr(NGram, \"get_perplexity\", get_perplexity)\n",
    "\n",
    "# Example tests (values depend on the actual dataset used)\n",
    "ng2 = NGram(3, is_character_based=False)\n",
    "ng2.set_data(read_lines(DATA_PATH, 2000, 0))  # fewer lines for quick demo\n",
    "ng2.generate_ngram_probabilities()\n",
    "print(\"Perplexity examples:\")\n",
    "for s in [\"is a great cd\", \"is a good cd\", \"is a bad cd\",\n",
    "          \"this music is a wonderful experience and i love it\"]:\n",
    "    print(s, \"->\", ng2.get_perplexity(s))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
