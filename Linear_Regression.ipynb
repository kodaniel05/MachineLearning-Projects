{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kJy2ESvii8T"
   },
   "source": [
    "# Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 17:09:59.251939: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-09-21 17:09:59.252034: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-09-21 17:09:59.536060: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-21 17:10:02.062725: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-21 17:10:25.915918: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import numpy as np\n",
    "import pandas as pd #I normally always import Pandas. I am not sure why you are not requiring it, since we have to read in the file right\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhgxwsNZe2be"
   },
   "source": [
    "##**Problem 1**\n",
    "In this problem, we will explore the basic linear regression: $y_n=w_0 +w_1x_n$, where $n=1,\\dots, N$ is the index of the data sample. Your task is to determine the appropriate values of $w_0$ and $w_1$ for the given data samples in Lab1_1.csv.\n",
    "\n",
    "Requirments:\n",
    "*   You are required to use gradient descent algorithm to complete this problem.\n",
    "*   You need to include the following four components in your lab report: (1) the codes, (2) the obtained appropriate value of $w_0$ and $w_1$, (3) the obtained training error, and (4) the obtained testing error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Lab1_1.csv\")\n",
    "xcol, ycol = df.select_dtypes(include=[np.number]).columns[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df, test_ratio=0.2, seed=1):\n",
    "    d = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    n_test = int(len(d)*test_ratio)\n",
    "    return d.iloc[n_test:].reset_index(drop=True), d.iloc[:n_test].reset_index(drop=True)\n",
    "\n",
    "train, test = split(df[[xcol, ycol]], test_ratio=0.2, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build design matrices: X = [1, x]\n",
    "X_tr = np.c_[np.ones(len(train)), train[xcol].to_numpy()]\n",
    "y_tr = train[ycol].to_numpy().reshape(-1, 1)\n",
    "X_te = np.c_[np.ones(len(test)),  test[xcol].to_numpy()]\n",
    "y_te = test[ycol].to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y, yhat): return float(np.mean((y - yhat)**2))\n",
    "\n",
    "def gd(X, y, lr=0.01, epochs=5000):\n",
    "    N, D = X.shape\n",
    "    w = np.zeros((D,1))\n",
    "    history = []\n",
    "    step = max(1, epochs//100)\n",
    "    for t in range(epochs):\n",
    "        yhat = X @ w\n",
    "        grad = (2.0/N) * (X.T @ (yhat - y))\n",
    "        w -= lr * grad\n",
    "        if t % step == 0 or t == epochs-1:\n",
    "            history.append(mse(y, yhat))\n",
    "    return w, history\n",
    "\n",
    "w, hist = gd(X_tr, y_tr, lr=0.01, epochs=5000)\n",
    "w0, w1 = float(w[0,0]), float(w[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_pred, te_pred = X_tr @ w, X_te @ w\n",
    "tr_mse, te_mse = mse(y_tr, tr_pred), mse(y_te, te_pred)\n",
    "\n",
    "print(\"Results\")\n",
    "print(f\"Columns used: x='{xcol}', y='{ycol}'\")\n",
    "print(f\"w0 (intercept): {w0:.6f}\")\n",
    "print(f\"w1 (slope):     {w1:.6f}\")\n",
    "print(f\"Training MSE:   {tr_mse:.6f}\")\n",
    "print(f\"Testing MSE:    {te_mse:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(); plt.plot(range(len(hist)), hist); plt.title(\"GD Convergence (MSE)\"); plt.xlabel(\"Checkpoint\"); plt.ylabel(\"MSE\"); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(df[xcol], df[ycol], s=14, label=\"data\")\n",
    "xs = np.linspace(df[xcol].min(), df[xcol].max(), 300)\n",
    "ys = w0 + w1*xs\n",
    "plt.plot(xs, ys, label=\"fit\")\n",
    "plt.title(\"Problem 1: Data & Learned Line\"); plt.xlabel(xcol); plt.ylabel(ycol); plt.legend(); plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gbg4e_BqlSbd"
   },
   "source": [
    "##**Problem 2**\n",
    "In this problem, we will explore an extended linear regression: $y_n=w_0 +w_1x_n+w_2x_n^2$, where $n=1,\\dots, N$ is the index of the data sample. Your task is to determine the appropriate values of $w_0$, $w_1$, and $w_2$ for the given data samples in Lab1_2.csv.\n",
    "\n",
    "Requirments:\n",
    "*   You are required to use gradient descent algorithm to complete this problem.\n",
    "*   You need to include the following four components in your lab report: (1) the codes, (2) the obtained appropriate value of $w_0$, $w_1$, and $w_2$, (3) the obtained training error, and (4) the obtained testing error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"Lab1_2.csv\")\n",
    "xcol, ycol = df2.select_dtypes(include=[np.number]).columns[:2]  # e.g., \"x\", \"y\"\n",
    "print(f\"Using columns -> x: '{xcol}', y: '{ycol}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df, test_ratio=0.2, seed=2):\n",
    "    d = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    n_test = int(len(d)*test_ratio)\n",
    "    return d.iloc[n_test:].reset_index(drop=True), d.iloc[:n_test].reset_index(drop=True)\n",
    "\n",
    "train2, test2 = split(df2[[xcol, ycol]], test_ratio=0.2, seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = train2[xcol].to_numpy()\n",
    "x_te = test2[xcol].to_numpy()\n",
    "\n",
    "X_tr = np.c_[np.ones(len(x_tr)), x_tr, x_tr**2]\n",
    "y_tr = train2[ycol].to_numpy().reshape(-1, 1)\n",
    "\n",
    "X_te = np.c_[np.ones(len(x_te)), x_te, x_te**2]\n",
    "y_te = test2[ycol].to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y, yhat): return float(np.mean((y - yhat)**2))\n",
    "\n",
    "def gd(X, y, lr=0.01, epochs=8000):\n",
    "    N, D = X.shape\n",
    "    w = np.zeros((D,1))\n",
    "    history = []\n",
    "    step = max(1, epochs//100)\n",
    "    for t in range(epochs):\n",
    "        yhat = X @ w\n",
    "        grad = (2.0/N) * (X.T @ (yhat - y))\n",
    "        w -= lr * grad\n",
    "        if t % step == 0 or t == epochs-1:\n",
    "            history.append(mse(y, yhat))\n",
    "    return w, history\n",
    "\n",
    "w, hist = gd(X_tr, y_tr, lr=0.01, epochs=8000)\n",
    "w0, w1, w2 = (float(w[i,0]) for i in range(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_pred, te_pred = X_tr @ w, X_te @ w\n",
    "tr_mse, te_mse = mse(y_tr, tr_pred), mse(y_te, te_pred)\n",
    "\n",
    "print(\"=== Problem 2 Results ===\")\n",
    "print(f\"w0 (bias):  {w0:.6f}\")\n",
    "print(f\"w1 (x):     {w1:.6f}\")\n",
    "print(f\"w2 (x^2):   {w2:.6f}\")\n",
    "print(f\"Training MSE: {tr_mse:.6f}\")\n",
    "print(f\"Testing MSE:  {te_mse:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(); plt.plot(range(len(hist)), hist)\n",
    "plt.title(\"Problem 2: GD Convergence (MSE)\"); plt.xlabel(\"Checkpoint\"); plt.ylabel(\"MSE\"); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(df2[xcol], df2[ycol], s=14, label=\"data\")\n",
    "xs = np.linspace(df2[xcol].min(), df2[xcol].max(), 400)\n",
    "ys = w0 + w1*xs + w2*(xs**2)\n",
    "plt.plot(xs, ys, label=\"quadratic fit\")\n",
    "plt.title(\"Problem 2: Data & Learned Quadratic\"); plt.xlabel(xcol); plt.ylabel(ycol); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"parameter\":[\"w0\",\"w1\",\"w2\",\"train_MSE\",\"test_MSE\"],\n",
    "              \"value\":[w0, w1, w2, tr_mse, te_mse]}).to_csv(\"problem2_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZo-9XHUVB3Y"
   },
   "source": [
    "##**Problem 3**\n",
    "In this problem, we will explore to use extended linear regression: $y_n=w_0 +\\sum_{k=1}^Kw_kx_{n,k}$ to solve a real-world problem on stock forecasting. Your task is predict the Close value based on the Open, High, and Low values given in Lab1_3.csv.  \n",
    "Requirments:\n",
    "*   You are required to use gradient descent algorithm to complete this problem.\n",
    "*   You need to include the following four components in your lab report: (1) the codes, (2) the obtained appropriate value of $w_0$, $w_1$, ..., $w_K$ , (3) the obtained training error, and (4) the obtained testing error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"Lab1_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = [\"Open\", \"High\", \"Low\", \"Close\"]\n",
    "if all(c in df3.columns for c in expected):\n",
    "    feature_cols = [\"Open\", \"High\", \"Low\"]\n",
    "    target_col = \"Close\"\n",
    "else:\n",
    "    num_cols = df3.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    target_col = \"Close\" if \"Close\" in df3.columns else (num_cols[-1])\n",
    "    feature_cols = [c for c in num_cols if c != target_col]\n",
    "print(\"Features:\", feature_cols, \"| Target:\", target_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(d, test_ratio=0.2, seed=3):\n",
    "    d = d.sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
    "    n_test = int(len(d)*test_ratio)\n",
    "    return d.iloc[n_test:].reset_index(drop=True), d.iloc[:n_test].reset_index(drop=True)\n",
    "\n",
    "train3, test3 = split(df3[feature_cols + [target_col]], test_ratio=0.2, seed=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = train3[feature_cols].mean()\n",
    "sd = train3[feature_cols].std(ddof=0).replace(0, 1.0)  # avoid divide-by-zero\n",
    "Xtr = ((train3[feature_cols] - mu) / sd).to_numpy()\n",
    "Xte = ((test3[feature_cols]  - mu) / sd).to_numpy()\n",
    "ytr = train3[target_col].to_numpy().reshape(-1,1)\n",
    "yte = test3[target_col].to_numpy().reshape(-1,1)\n",
    "\n",
    "Xtr = np.c_[np.ones(len(Xtr)), Xtr]\n",
    "Xte = np.c_[np.ones(len(Xte)), Xte]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y, yhat): return float(np.mean((y - yhat)**2))\n",
    "\n",
    "def gd(X, y, lr=0.01, epochs=20000):\n",
    "    N, D = X.shape\n",
    "    w = np.zeros((D,1))\n",
    "    history = []\n",
    "    step = max(1, epochs//200)\n",
    "    for t in range(epochs):\n",
    "        yhat = X @ w\n",
    "        grad = (2.0/N) * (X.T @ (yhat - y))\n",
    "        w -= lr * grad\n",
    "        if t % step == 0 or t == epochs-1:\n",
    "            history.append(mse(y, yhat))\n",
    "    return w, history\n",
    "\n",
    "w_scaled, hist = gd(Xtr, ytr, lr=0.01, epochs=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_pred = Xtr @ w_scaled\n",
    "te_pred = Xte @ w_scaled\n",
    "tr_mse = mse(ytr, tr_pred)\n",
    "te_mse = mse(yte, te_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_s = float(w_scaled[0,0])\n",
    "wk_s = w_scaled[1:,0]\n",
    "wk_orig = wk_s / sd.to_numpy()\n",
    "w0_orig = w0_s - float((wk_s * (mu.to_numpy()/sd.to_numpy())).sum())\n",
    "\n",
    "print(\"\\nResults\")\n",
    "print(f\"w0 (intercept, original scale): {w0_orig:.6f}\")\n",
    "for i, col in enumerate(feature_cols):\n",
    "    print(f\"w({col}) : {wk_orig[i]:.6f}\")\n",
    "print(f\"Training MSE: {tr_mse:.6f}\")\n",
    "print(f\"Testing  MSE: {te_mse:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(len(hist)), hist)\n",
    "plt.title(\"Problem 3: GD Convergence (MSE)\")\n",
    "plt.xlabel(\"Checkpoint\"); plt.ylabel(\"MSE\"); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(yte, te_pred, s=14)\n",
    "plt.plot([yte.min(), yte.max()], [yte.min(), yte.max()])\n",
    "plt.title(\"Problem 3: Test Predictions vs Actual (Close)\")\n",
    "plt.xlabel(\"Actual Close\"); plt.ylabel(\"Predicted Close\"); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_rows = [{\"parameter\":\"w0\",\"value\":w0_orig}]\n",
    "out_rows += [{\"parameter\":f\"w({c})\",\"value\":float(wk_orig[i])} for i,c in enumerate(feature_cols)]\n",
    "out_rows += [{\"parameter\":\"train_MSE\",\"value\":tr_mse},{\"parameter\":\"test_MSE\",\"value\":te_mse}]\n",
    "pd.DataFrame(out_rows).to_csv(\"problem3_summary.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (tensorflow-env)",
   "language": "python",
   "name": "tensorflow-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
