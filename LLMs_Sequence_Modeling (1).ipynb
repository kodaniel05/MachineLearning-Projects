{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7412f0d9",
   "metadata": {},
   "source": [
    "\n",
    "# Large Language Models (LLMs): Sequence Modeling â€” Ready-to-Run Notebook\n",
    "\n",
    "This notebook contains everything you need for the **Sequence Modeling** project:\n",
    "- Helpers (`read_lines`, `clean_text`)\n",
    "- Minimal `NGram` class and `get_next_word_loop`\n",
    "- FastText embedding training (`generate_fasttext_vectors`)\n",
    "- Dataset builder for RNN/LSTM (`create_dataset`)\n",
    "- Utilities to map embeddings back to words\n",
    "- **RNN** and **LSTM** PyTorch models + training & prediction helpers\n",
    "\n",
    "> **Dataset path:** `/anvil/projects/tdm/data/amazon/music.txt`  \n",
    "> You can reduce the number of lines to speed things up while testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f09a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, time, codecs, numpy as np\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gensim (word vectors)\n",
    "from gensim.models import FastText, KeyedVectors\n",
    "\n",
    "# Torch (RNN/LSTM)\n",
    "import torch\n",
    "from torch import nn, optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dab1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_lines(file_path: str, n: int, start: int = 0) -> List[str]:\n",
    "    \"\"\"Read `n` lines from `file_path` starting at line index `start` (0-based).\"\"\"\n",
    "    lines = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i < start:\n",
    "                continue\n",
    "            lines.append(line.strip())\n",
    "            if len(lines) == n:\n",
    "                break\n",
    "    return lines\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Decode escapes, normalize whitespace/newlines, lowercase, keep alpha+space.\"\"\"\n",
    "    text = codecs.decode(text, 'unicode_escape', errors='ignore')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.lower()\n",
    "    text = ''.join(c for c in text if c.isalpha() or c.isspace())\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46854eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NGram:\n",
    "    def __init__(self, n: int, is_character_based: bool = False):\n",
    "        self.n = n\n",
    "        self.is_character_based = is_character_based\n",
    "        self.data = None\n",
    "        self.ngram_frequencies: Dict[Any, int] = {}\n",
    "        self.ngram_probabilities: Dict[Any, Dict[Any, float]] = {}\n",
    "\n",
    "    def set_data(self, data: List[str]):\n",
    "        if not isinstance(data, list):\n",
    "            return\n",
    "        cleaned = [clean_text(s) for s in data]\n",
    "        if self.is_character_based:\n",
    "            # list of strings\n",
    "            self.data = [s for s in cleaned if s]\n",
    "        else:\n",
    "            # list of tuples of words for hashability\n",
    "            self.data = [tuple(s.split()) for s in cleaned if s]\n",
    "\n",
    "    def generate_ngrams(self) -> Dict[Any, int]:\n",
    "        self.ngram_frequencies = {}\n",
    "        if not isinstance(self.data, list) or len(self.data) == 0:\n",
    "            return {}\n",
    "        for item in self.data:\n",
    "            seq = item if self.is_character_based else list(item)\n",
    "            for i in range(len(seq) - self.n + 1):\n",
    "                ngram = ''.join(seq[i:i+self.n]) if self.is_character_based else tuple(seq[i:i+self.n])\n",
    "                self.ngram_frequencies[ngram] = self.ngram_frequencies.get(ngram, 0) + 1\n",
    "        return self.ngram_frequencies\n",
    "\n",
    "    def generate_ngram_probabilities(self) -> Dict[Any, Dict[Any, float]]:\n",
    "        freqs = self.generate_ngrams()\n",
    "        self.ngram_probabilities = {}\n",
    "        if not isinstance(freqs, dict) or len(freqs) == 0:\n",
    "            return {}\n",
    "        for ngram, count in freqs.items():\n",
    "            prefix = ngram[:-1] if not self.is_character_based else ngram[:-1]\n",
    "            last = ngram[-1] if not self.is_character_based else ngram[-1]\n",
    "            self.ngram_probabilities.setdefault(prefix, {})\n",
    "            self.ngram_probabilities[prefix][last] = self.ngram_probabilities[prefix].get(last, 0) + count\n",
    "        # normalize\n",
    "        for prefix, nexts in self.ngram_probabilities.items():\n",
    "            total = float(sum(nexts.values()))\n",
    "            for k in list(nexts.keys()):\n",
    "                nexts[k] = nexts[k] / total if total > 0 else 0.0\n",
    "        return self.ngram_probabilities\n",
    "\n",
    "    def get_next_word(self, previous_words: str, method: str = 'common') -> str:\n",
    "        if not isinstance(self.ngram_probabilities, dict) or len(self.ngram_probabilities) == 0:\n",
    "            return ''\n",
    "        if self.is_character_based:\n",
    "            key = previous_words[-(self.n-1):]\n",
    "        else:\n",
    "            toks = previous_words.split()\n",
    "            if len(toks) != self.n - 1:\n",
    "                return ''\n",
    "            key = tuple(toks)\n",
    "        if key not in self.ngram_probabilities:\n",
    "            return ''\n",
    "        options = self.ngram_probabilities[key]\n",
    "        if method == 'random':\n",
    "            choices = list(options.keys())\n",
    "            probs = list(options.values())\n",
    "            return np.random.choice(choices, p=probs)\n",
    "        elif method == 'uncommon':\n",
    "            return min(options.items(), key=lambda x: x[1])[0]\n",
    "        else: # 'common'\n",
    "            return max(options.items(), key=lambda x: x[1])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70177d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_next_word_loop(model: NGram, start_string: str, n: int = 10, method: str = 'common') -> str:\n",
    "    s = start_string.strip()\n",
    "    for _ in range(n):\n",
    "        nxt = model.get_next_word(' '.join(s.split()[-(model.n-1):]), method=method)\n",
    "        if not nxt:\n",
    "            break\n",
    "        s += ' ' + nxt\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cf97b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_fasttext_vectors(lines: List[str], filename: str) -> KeyedVectors:\n",
    "    cleaned = [clean_text(line) for line in lines]\n",
    "    tokenized = [line.split() for line in cleaned if line]\n",
    "    model = FastText(tokenized, vector_size=100, window=5, min_count=1, workers=1)\n",
    "    model.save(f'{filename}.model')\n",
    "    word_vectors = model.wv\n",
    "    word_vectors.save(f'{filename}.wordvectors')\n",
    "    return word_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcdd421",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataset(data: List[str], word_embeddings: KeyedVectors, n: int = 3):\n",
    "    input_data, output_data = [], []\n",
    "    all_words, all_embeddings = [], []\n",
    "\n",
    "    for line in data:\n",
    "        line_clean = clean_text(line)\n",
    "        words = line_clean.split()\n",
    "        if len(words) < n:\n",
    "            continue\n",
    "\n",
    "        embeddings = []\n",
    "        for w in words:\n",
    "            if w in word_embeddings:\n",
    "                embeddings.append(word_embeddings[w])\n",
    "                all_words.append(w)\n",
    "                all_embeddings.append(word_embeddings[w])\n",
    "\n",
    "        if len(embeddings) < n:\n",
    "            continue\n",
    "\n",
    "        # groups of length n\n",
    "        embedding_groups = [embeddings[i:i+n] for i in range(len(embeddings) - n + 1)]\n",
    "\n",
    "        r_i, r_o = [], []\n",
    "        for group in embedding_groups:\n",
    "            prev_seq = np.array(group[:-1], dtype=np.float32)  # (n-1, 100)\n",
    "            target = np.array([group[-1]], dtype=np.float32)   # (1, 100)\n",
    "            r_i.append(prev_seq)\n",
    "            r_o.append(target)\n",
    "\n",
    "        if r_i and r_o:\n",
    "            input_data.append(np.array(r_i, dtype=np.float32))   # (num_seq, n-1, 100)\n",
    "            output_data.append(np.array(r_o, dtype=np.float32))  # (num_seq, 1, 100)\n",
    "\n",
    "    return (all_words, all_embeddings, input_data, output_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99459aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cosine_similarity_vec(v1: np.ndarray, v2: np.ndarray) -> float:\n",
    "    dot = np.dot(v1, v2)\n",
    "    n1 = np.linalg.norm(v1)\n",
    "    n2 = np.linalg.norm(v2)\n",
    "    if n1 == 0 or n2 == 0:\n",
    "        return 0.0\n",
    "    return float(dot / (n1 * n2))\n",
    "\n",
    "def get_word_from_embedding(embedding: np.ndarray, keyedvectors: KeyedVectors):\n",
    "    best_word, best_sim = None, -1.0\n",
    "    for w in keyedvectors.index_to_key:\n",
    "        sim = cosine_similarity_vec(embedding, keyedvectors[w])\n",
    "        if sim > best_sim:\n",
    "            best_sim, best_word = sim, w\n",
    "    return (best_word, best_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b62da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size=100, hidden_size=128, output_size=100, input_sequence_length=2):\n",
    "        super().__init__()\n",
    "        self.input_sequence_length = input_sequence_length\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])  # take last time step\n",
    "        return out, hidden\n",
    "\n",
    "    def train_model(self, input_data, output_data, num_epochs=3, learning_rate=0.001):\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0.0\n",
    "            s = time.time()\n",
    "            for i, (r_i, r_o) in enumerate(zip(input_data, output_data)):\n",
    "                if i % 500 == 0:\n",
    "                    print(f'Processing review {i+1}/{len(input_data)}')\n",
    "                    print(f'Elapsed time: {time.time() - s:.2f}s'); s = time.time()\n",
    "\n",
    "                hidden_state = None  # reset per review\n",
    "                for seq, o in zip(r_i, r_o):\n",
    "                    input_tensor = torch.tensor(seq, dtype=torch.float32).unsqueeze(0)  # [1, seq_len, 100]\n",
    "                    target_tensor = torch.tensor(o[0], dtype=torch.float32)             # [100]\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    output, hidden_state = self.forward(input_tensor, hidden_state)\n",
    "                    output = output[-1]  # [100]\n",
    "\n",
    "                    if hidden_state is not None:\n",
    "                        hidden_state = hidden_state.detach()\n",
    "\n",
    "                    loss = criterion(output, target_tensor)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(input_data):.4f}')\n",
    "        return total_loss/len(input_data)\n",
    "\n",
    "def predict_from_string(input_string: str, model: RNNModel, keyedvectors: KeyedVectors):\n",
    "    clean = clean_text(input_string)\n",
    "    words = clean.split()\n",
    "    if len(words) < model.input_sequence_length:\n",
    "        return ('', 0.0)\n",
    "\n",
    "    input_embeddings = [keyedvectors[w] for w in words if w in keyedvectors]\n",
    "    if len(input_embeddings) < model.input_sequence_length:\n",
    "        return ('', 0.0)\n",
    "\n",
    "    seqs = [input_embeddings[i:i+model.input_sequence_length] for i in range(len(input_embeddings) - model.input_sequence_length + 1)]\n",
    "    hidden = None\n",
    "    out = None\n",
    "    for seq in seqs:\n",
    "        input_tensor = torch.tensor(seq, dtype=torch.float32).unsqueeze(0)\n",
    "        out, hidden = model(input_tensor, hidden)\n",
    "        if hidden is not None:\n",
    "            hidden = hidden.detach()\n",
    "\n",
    "    pred_emb = out.detach().numpy()[0]  # [100]\n",
    "    return get_word_from_embedding(pred_emb, keyedvectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb0a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=100, hidden_size=128, output_size=100, input_sequence_length=2):\n",
    "        super().__init__()\n",
    "        self.input_sequence_length = input_sequence_length\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out, hidden\n",
    "\n",
    "    def train_model(self, input_data, output_data, num_epochs=3, learning_rate=0.001):\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0.0\n",
    "            s = time.time()\n",
    "            for i, (r_i, r_o) in enumerate(zip(input_data, output_data)):\n",
    "                if i % 500 == 0:\n",
    "                    print(f'Processing review {i+1}/{len(input_data)}')\n",
    "                    print(f'Elapsed time: {time.time() - s:.2f}s'); s = time.time()\n",
    "\n",
    "                hidden = None\n",
    "                for seq, o in zip(r_i, r_o):\n",
    "                    input_tensor = torch.tensor(seq, dtype=torch.float32).unsqueeze(0)  # [1, seq_len, 100]\n",
    "                    target_tensor = torch.tensor(o[0], dtype=torch.float32)             # [100]\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    output, hidden = self.forward(input_tensor, hidden)\n",
    "                    output = output[-1]\n",
    "\n",
    "                    if hidden is not None:\n",
    "                        hidden = (hidden[0].detach(), hidden[1].detach())\n",
    "\n",
    "                    loss = criterion(output, target_tensor)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(input_data):.4f}')\n",
    "        return total_loss/len(input_data)\n",
    "\n",
    "def predict_from_string_lstm(input_string: str, model: LSTMModel, keyedvectors: KeyedVectors):\n",
    "    clean = clean_text(input_string)\n",
    "    words = clean.split()\n",
    "    if len(words) < model.input_sequence_length:\n",
    "        return ('', 0.0)\n",
    "    input_embeddings = [keyedvectors[w] for w in words if w in keyedvectors]\n",
    "    if len(input_embeddings) < model.input_sequence_length:\n",
    "        return ('', 0.0)\n",
    "\n",
    "    seqs = [input_embeddings[i:i+model.input_sequence_length] for i in range(len(input_embeddings) - model.input_sequence_length + 1)]\n",
    "    hidden = None\n",
    "    out = None\n",
    "    for seq in seqs:\n",
    "        input_tensor = torch.tensor(seq, dtype=torch.float32).unsqueeze(0)\n",
    "        out, hidden = model(input_tensor, hidden)\n",
    "        if hidden is not None:\n",
    "            hidden = (hidden[0].detach(), hidden[1].detach())\n",
    "\n",
    "    pred_emb = out.detach().numpy()[0]\n",
    "    return get_word_from_embedding(pred_emb, keyedvectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10090f6",
   "metadata": {},
   "source": [
    "\n",
    "## Quick-start demo (optional)\n",
    "\n",
    "> This section trains very small toy runs so you can verify everything works.  \n",
    "> For better results, increase the number of lines and training epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86f4f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_PATH = '/anvil/projects/tdm/data/amazon/music.txt'\n",
    "\n",
    "# --- Tiny NGram demo ---\n",
    "ng = NGram(3, is_character_based=False)\n",
    "ng.set_data(read_lines(DATA_PATH, 2000, 0))\n",
    "ng.generate_ngram_probabilities()\n",
    "print(get_next_word_loop(ng, \"this is a\", n=5))\n",
    "\n",
    "# --- Train small FastText & build dataset ---\n",
    "wv = generate_fasttext_vectors(read_lines(DATA_PATH, 3000, 0), 'P12_fasttext_small')\n",
    "words, embs, X, Y = create_dataset(read_lines(DATA_PATH, 1200, 500), wv, n=3)\n",
    "\n",
    "print(len(X), len(Y))\n",
    "if len(X) > 5:\n",
    "    print(X[0].shape, X[1].shape)\n",
    "\n",
    "# --- Train a very small RNN (few epochs) ---\n",
    "if X:\n",
    "    rnn = RNNModel(input_size=100, hidden_size=64, output_size=100, input_sequence_length=2)\n",
    "    rnn.train_model(X, Y, num_epochs=1, learning_rate=0.001)\n",
    "    print(predict_from_string(\"this is a wonderful cd and\", rnn, wv))\n",
    "\n",
    "# --- Train a very small LSTM (few epochs) ---\n",
    "if X:\n",
    "    lstm = LSTMModel(input_size=100, hidden_size=64, output_size=100, input_sequence_length=2)\n",
    "    lstm.train_model(X, Y, num_epochs=1, learning_rate=0.001)\n",
    "    print(predict_from_string_lstm(\"this is a wonderful cd and\", lstm, wv))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
